{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8bd079e-9875-47ba-9b82-06a447aa9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen parameter: features.0.weight, requires_grad: False\n",
      "Frozen parameter: features.0.bias, requires_grad: False\n",
      "Frozen parameter: features.3.weight, requires_grad: False\n",
      "Frozen parameter: features.3.bias, requires_grad: False\n",
      "Frozen parameter: features.6.weight, requires_grad: False\n",
      "Frozen parameter: features.6.bias, requires_grad: False\n",
      "Frozen parameter: features.8.weight, requires_grad: False\n",
      "Frozen parameter: features.8.bias, requires_grad: False\n",
      "Frozen parameter: features.11.weight, requires_grad: False\n",
      "Frozen parameter: features.11.bias, requires_grad: False\n",
      "Frozen parameter: features.13.weight, requires_grad: False\n",
      "Frozen parameter: features.13.bias, requires_grad: False\n",
      "Frozen parameter: embeddings.0.weight, requires_grad: False\n",
      "Frozen parameter: embeddings.0.bias, requires_grad: False\n",
      "Frozen parameter: embeddings.2.weight, requires_grad: False\n",
      "Frozen parameter: embeddings.2.bias, requires_grad: False\n",
      "Frozen parameter: embeddings.4.weight, requires_grad: False\n",
      "Frozen parameter: embeddings.4.bias, requires_grad: False\n",
      "Total trainable parameters: 8321\n",
      "\n",
      "Epoch 1/2\n",
      "Processing batch 1\n",
      "Processing batch 2\n",
      "Processing batch 3\n",
      "\n",
      "Epoch 2/2\n",
      "Processing batch 1\n",
      "Processing batch 2\n",
      "Processing batch 3\n",
      "\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Minor       0.00      0.00      0.00         5\n",
      "       Major       0.44      1.00      0.62         4\n",
      "\n",
      "    accuracy                           0.44         9\n",
      "   macro avg       0.22      0.50      0.31         9\n",
      "weighted avg       0.20      0.44      0.27         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchvggish\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class ChordDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, sample_rate=16000):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # VGGish expects 96 mel bands\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            n_mels=96,\n",
    "            f_min=125,\n",
    "            f_max=7500\n",
    "        )\n",
    "        \n",
    "        # Log mel spectrogram\n",
    "        self.amplitude_to_db = T.AmplitudeToDB()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(self.file_paths[idx])\n",
    "        #print(f\"Getting: {self.file_paths[idx]}\")\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = T.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Get mel spectrogram\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        # Convert to dB scale\n",
    "        mel_spec = self.amplitude_to_db(mel_spec)\n",
    "        #print(f\"Getting mel_spec: {mel_spec}\")\n",
    "        \n",
    "        # VGGish expects input size of (batch_size, 1, 96, 64)\n",
    "        # So we need to ensure our time dimension is 64 frames\n",
    "        target_length = 64\n",
    "        current_length = mel_spec.size(2)\n",
    "        \n",
    "        if current_length < target_length:\n",
    "            # Pad if too short\n",
    "            padding = target_length - current_length\n",
    "            mel_spec = torch.nn.functional.pad(mel_spec, (0, padding))\n",
    "            \n",
    "        elif current_length > target_length:\n",
    "            # Take center portion if too long\n",
    "            start = (current_length - target_length) // 2\n",
    "            mel_spec = mel_spec[:, :, start:start + target_length]\n",
    "        \n",
    "        # Add channel dimension\n",
    "       # mel_spec = mel_spec.unsqueeze(0)\n",
    "            \n",
    "        return mel_spec, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class ChordClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = torchvggish.vggish()\n",
    "\n",
    "        # Freeze VGGish parameters\n",
    "        for name, param in self.feature_extractor.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            print(f\"Frozen parameter: {name}, requires_grad: {param.requires_grad}\")\n",
    "        \n",
    "            \n",
    "        # Simple classifier on top of VGGish embeddings\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),  # VGGish outputs 128-dimensional embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove torch.no_grad() here\n",
    "        features = self.feature_extractor(x)  # Remove torch.no_grad() here\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "class ChordTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, device, threshold=0.5):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "            # Verify parameters are unfrozen\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            print(\"Processing batch\", i+1)\n",
    "            \n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            #for name, param in model.named_parameters():\n",
    "             #   if param.requires_grad:\n",
    "              #      print(f\"Trainable parameter: {name}, shape: {param.shape}\")\n",
    "            \n",
    "                        \n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                avg_loss = running_loss / 10\n",
    "                print(f'Batch {i+1}, Loss: {avg_loss:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    \n",
    "                \n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predicted = (outputs.squeeze() > self.threshold).float()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        return classification_report(\n",
    "            all_labels, \n",
    "            all_preds, \n",
    "            target_names=[\"Minor\", \"Major\"], \n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "def get_dataloader(file_paths, labels, batch_size=16, shuffle=True):\n",
    "    dataset = ChordDataset(file_paths, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    file_dir = r'C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw'\n",
    "    \n",
    "    # Prepare data\n",
    "    file_paths = [os.path.join(file_dir, f) for f in os.listdir(file_dir) if f.endswith('.wav')]\n",
    "    labels = [0 if 'Minor' in f else 1 for f in os.listdir(file_dir) if f.endswith('.wav')]\n",
    "    \n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        file_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    #Changed for train set evaluation!\n",
    "    train_dataloader = get_dataloader(file_paths, labels)\n",
    "    test_dataloader = get_dataloader(test_paths, test_labels)\n",
    "    \n",
    "        # Initialize model\n",
    "    model = ChordClassifier().to(device)\n",
    "    \n",
    "    # Check trainable parameters\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_trainable_params}\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.2)\n",
    "    trainer = ChordTrainer(model, criterion, optimizer, device, threshold=0.2)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 2\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        trainer.train_epoch(train_dataloader)\n",
    "        \n",
    "    # Evaluation\n",
    "    report = trainer.evaluate(test_dataloader)\n",
    "    print('\\nTest Results:')\n",
    "    print(report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2522d19-3674-4bec-85f0-1684d0fd41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ChordDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths  # List of file paths to .wav files\n",
    "        self.labels = labels  # List of labels (0 for minor, 1 for major)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(file_path)  # Load the .wav file\n",
    "        \n",
    "        # Convert waveform to Mel-spectrogram\n",
    "        mel_spec_transform = T.MelSpectrogram(sample_rate=sample_rate, n_mels=64)\n",
    "        mel_spectrogram = mel_spec_transform(waveform)\n",
    "        \n",
    "        # Add a channel dimension for the CNN input\n",
    "        mel_spectrogram = mel_spectrogram\n",
    "        \n",
    "        if self.transform:\n",
    "            mel_spectrogram = self.transform(mel_spectrogram)\n",
    "        \n",
    "        return mel_spectrogram, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Example usage for DataLoader\n",
    "def get_dataloader(file_paths, labels, batch_size=16, shuffle=True):\n",
    "    dataset = ChordDataset(file_paths, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4acea84-b3f3-459f-bd6b-4b58cca834e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvggish' has no attribute 'download_vggish_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Instantiate the VGGish model (pretrained)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m torchvggish\u001b[38;5;241m.\u001b[39mdownload_vggish_weights()  \u001b[38;5;66;03m# Download VGGish pretrained weights\u001b[39;00m\n\u001b[0;32m     35\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m torchvggish\u001b[38;5;241m.\u001b[39mVGGish()  \u001b[38;5;66;03m# Load pretrained VGGish model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Instantiate your classifier with the pretrained model as an intermediate layer\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvggish' has no attribute 'download_vggish_weights'"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, num_epochs=10, learning_rate=0.2):\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            # Move data to the appropriate device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:  # Print every 10 mini-batches\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 10:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Example device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the VGGish model (pretrained)\n",
    "torchvggish.download_vggish_weights()  # Download VGGish pretrained weights\n",
    "pretrained_model = torchvggish.VGGish()  # Load pretrained VGGish model\n",
    "\n",
    "# Instantiate your classifier with the pretrained model as an intermediate layer\n",
    "model = ChordClassifier(pretrained_model).to(device)\n",
    "# Continue with training, evaluation, etc., as per your previous implementation\n",
    "\n",
    "# Continue with training, evaluation, etc., as per your previous implementation\n",
    "\n",
    "# Define the path to the directory containing the .wav files\n",
    "file_dir = r'C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw'\n",
    "\n",
    "# Get a list of all .wav files in the directory\n",
    "file_paths = [os.path.join(file_dir, f) for f in os.listdir(file_dir) if f.endswith('.wav')]\n",
    "\n",
    "labels = [0 if 'Minor' in f else 1 for f in os.listdir(file_dir) if f.endswith('.wav')]  # 0 for Minor, 1 for Major\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Get the DataLoader for training and testing sets\n",
    "train_dataloader = get_dataloader(train_paths, train_labels)\n",
    "test_dataloader = get_dataloader(test_paths, test_labels)\n",
    "\n",
    "# Train the model on the training data\n",
    "train_model(model, train_dataloader, num_epochs=2, learning_rate=0.2)\n",
    "\n",
    "# Evaluate the model on the test data using the evaluate method from before\n",
    "model.evaluate(test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef312df-6bc0-436b-b7db-2eafbdc375c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfad214-ff4b-46c3-8cba-5ef573eee0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
